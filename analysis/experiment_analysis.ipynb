{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Analyse the Results of Running Moran Process Experiment on Different Graphs\n",
    "This is the newest version of this analysis file, where I can merge the csv of different jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/labs/pilpel/matanyaw/moran-process \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from analysis.analysis_utils import plot_hybrid_density, aggregate_results_no_load, GRAPH_PROPERTY_DESCRIPTION, COLOR_DICT\n",
    "# change this if on a different computer!\n",
    "from population_graph import GRAPH_PROPS\n",
    "# Set aesthetic parameters for \"publication-quality\" plots\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "plt.rcParams['lines.linewidth'] = 2.5\n",
    "\n",
    "BATCH_NAME = 'merged_batch_04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(os.getcwd()) \n",
    "\n",
    "# Now define your paths relative to ROOT\n",
    "BATCH_DIR = ROOT / \"simulation_data\" / BATCH_NAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "output_file = os.path.join(batch_dir, f\"temp_full_results.csv\")\n",
    "tmp_results_path = os.path.join(batch_dir, \"tmp\", \"results\")\n",
    "all_files = glob.glob(os.path.join(tmp_results_path, \"result_job_*.csv\"))\n",
    "print(f\"Found {len(all_files)} files in temp results directory: {tmp_results_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_path = aggregate_results_no_load(batch_dir=batch_dir, delete_temp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(results_df_path)\n",
    "print(\"columns: \", results_df.columns)\n",
    "print(\"shape: \", results_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column where steps are NaN if fixation failed\n",
    "# This allows .agg() to ignore those values automatically for median/std\n",
    "results_df['steps_success'] = results_df['steps'].where(results_df['fixation'] == True)\n",
    "\n",
    "analysis_df = results_df.groupby(['wl_hash', 'r', 'graph_name']).agg(\n",
    "    prob_fixation=('fixation', 'mean'),\n",
    "    median_steps=('steps_success', 'median'),\n",
    "    mean_steps=('steps_success', 'mean'),\n",
    "    std_steps=('steps_success', 'std'),\n",
    "    q25_steps=('steps_success', lambda x: x.quantile(0.25)),\n",
    "    q75_steps=('steps_success', lambda x: x.quantile(0.75)),\n",
    "    iqr_steps=('steps_success', lambda x: x.quantile(0.75) - x.quantile(0.25)),\n",
    "    n_grouped=('fixation', 'size')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Shape before merging: \", analysis_df.shape)\n",
    "# df_graphs = load_experiment_data('graph_database.csv')       # Graph database\n",
    "df_graphs = pd.read_csv(os.path.join(batch_dir, 'graph_props.csv'))\n",
    "\n",
    "# Merge with graph metadata\n",
    "analysis_df = pd.merge(\n",
    "    analysis_df, \n",
    "    df_graphs, \n",
    "    on=['wl_hash', 'graph_name'], \n",
    "    how='left', \n",
    "    suffixes=('', '_db')\n",
    ")\n",
    "# Sorting\n",
    "analysis_df['z_order'] = (analysis_df['category'] != 'Random').astype(int)\n",
    "analysis_df = analysis_df.sort_values('z_order').drop(columns='z_order')\n",
    "analysis_df.to_csv(os.path.join(batch_dir, 'graph_statistics.csv'), index=False)\n",
    "\n",
    "\n",
    "print(\"Shape after merging: \", analysis_df.shape)\n",
    "# Display sample\n",
    "analysis_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(analysis_df['mean_steps'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Mean Steps')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Mean Steps')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results_df and analysis_df on wl_hash and graph_name, excluding Random category\n",
    "merged_df = pd.merge(\n",
    "    results_df,\n",
    "    df_graphs[df_graphs['category'] != 'Random'],\n",
    "    on=['wl_hash', 'graph_name'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Filter out Random category graphs\n",
    "print(f\"Merged dataframe shape: {merged_df.shape}\")\n",
    "merged_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.analysis_utils import generate_robust_color_dict\n",
    "# --- Example Usage ---\n",
    "# Assuming 'analysis_df' is defined in your environment\n",
    "# (Creating dummy data for demonstration)\n",
    "\n",
    "categories = sorted(analysis_df['category'].dropna().unique().tolist())\n",
    "\n",
    "category_color_dict = generate_robust_color_dict(analysis_df, COLOR_DICT)\n",
    "\n",
    "print(\"Final Color Dictionary:\")\n",
    "for cat, color in category_color_dict.items():\n",
    "    print(f\"  {cat:15}: {color}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Setup the 3D figure\n",
    "fig = plt.figure(figsize=(12, 10)) \n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "# 2. Get the global min and max to make standard bins\n",
    "animal_results = merged_df.loc[merged_df['category'].isin(categories), 'steps_success'].dropna()\n",
    "bins = np.linspace(animal_results.min(), animal_results.max(), 50) \n",
    "\n",
    "spacing_factor = 2.5 \n",
    "\n",
    "# 3. Plot each category on a different depth axis (z-axis)\n",
    "for z_index, category in enumerate(categories):\n",
    "    data = merged_df.loc[merged_df['category'] == category, 'steps_success'].dropna()\n",
    "    \n",
    "    # Calculate the raw histogram counts\n",
    "    counts, _ = np.histogram(data, bins=bins)\n",
    "    \n",
    "    # --- NEW: Convert raw counts to percentage ---\n",
    "    # Divide by the total number of instances in this category, multiply by 100\n",
    "    if len(data) > 0:\n",
    "        hist_values = (counts / len(data)) * 100\n",
    "    else:\n",
    "        hist_values = counts # Fallback if a category is entirely empty\n",
    "    \n",
    "    x_coords = bins[:-1]\n",
    "    current_depth = z_index * spacing_factor\n",
    "\n",
    "    ax.bar(x_coords, \n",
    "           hist_values, \n",
    "           zs=current_depth,               \n",
    "           zdir='y',        \n",
    "           width=(bins[1] - bins[0]) * 0.9, \n",
    "           color=category_color_dict[category], \n",
    "           alpha=0.8,\n",
    "           edgecolor='black',\n",
    "           linewidth=0.5)\n",
    "\n",
    "# 4. Clean up the axes\n",
    "ax.set_xlabel('Steps to Fixation', labelpad=10)\n",
    "\n",
    "# --- NEW: Update the Z-axis label ---\n",
    "ax.set_zlabel('Percentage (%)', labelpad=10)\n",
    "\n",
    "ax.set_yticks([i * spacing_factor for i in range(len(categories))])\n",
    "ax.set_yticklabels(categories, rotation=-15, ha='left', va='center')\n",
    "\n",
    "ax.set_box_aspect(aspect=(1, 2, 1)) \n",
    "ax.view_init(elev=25, azim=-55)\n",
    "\n",
    "plt.title('3D Distribution of Steps to Fixation (Normalized)', pad=20)\n",
    "plt.subplots_adjust(left=0.05, right=0.85, bottom=0.1, top=0.95)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlaid histograms of steps_success by category\n",
    "animal_categories = [\"Mammalian\", \"Fish\", \"Avian\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define common bins based on all data\n",
    "animal_results = merged_df.loc[merged_df['category'].isin(categories), 'steps_success'].dropna()\n",
    "# bins = 50  # or use: bins = np.linspace(all_data.min(), all_data.max(), 51)\n",
    "bins = np.linspace(animal_results.min(), animal_results.max(), 101)\n",
    "\n",
    "for category in categories:\n",
    "    data = merged_df.loc[merged_df['category'] == category, 'steps_success'].dropna()\n",
    "    plt.hist(data, bins=bins, alpha=0.4, edgecolor='black', label=category, color=category_color_dict[category])\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Steps to Fixation by Category')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hybrid_density(analysis_df, \n",
    "                    'std_steps', \n",
    "                    'mean_steps', \n",
    "                    with_violin=False, \n",
    "                    color_dict=category_color_dict, \n",
    "                    highlight_categories=animal_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hybrid_density(analysis_df, 'max_degree', 'prob_fixation', with_violin=True, color_dict=category_color_dict, highlight_categories=animal_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot = analysis_df[analysis_df['r'] == 1.1]\n",
    "\n",
    "# NEW_GRAPH_PROPS = ['avg_degree', 'max_degree']\n",
    "print(GRAPH_PROPS)\n",
    "# plot_hybrid_density(df_to_plot, 'mean_steps', 'std_steps', with_violin=False)\n",
    "plot_hybrid_density(analysis_df, 'prob_fixation', 'mean_steps', with_violin=False, color_dict=category_color_dict, highlight_categories=animal_categories)\n",
    "plot_hybrid_density(analysis_df, 'mean_steps', 'prob_fixation', with_violin=False, color_dict=category_color_dict, highlight_categories=animal_categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Showing all plots on Mean Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- EXAMPLES OF USAGE ---\n",
    "for prop in GRAPH_PROPS:\n",
    "    # plot_property_effect(df_to_plot, prop, 'median_steps')\n",
    "    plot_hybrid_density(df_to_plot, prop, 'mean_steps', density_threshold=50, with_violin=True, color_dict=category_color_dict, highlight_categories=animal_categories)\n",
    "    # plot_hybrid_density(df_to_plot, prop, 'prob_fixation', density_threshold=50, with_violin=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Showing all plots on Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- EXAMPLES OF USAGE ---\n",
    "for prop in GRAPH_PROPS:\n",
    "    # plot_property_effect(df_to_plot, prop, 'median_steps')\n",
    "    # plot_hybrid_density(df_to_plot, prop, 'mean_steps', density_threshold=50, with_violin=True, color_dict=category_color_dict, highlight_categories=animal_categories)\n",
    "    plot_hybrid_density(df_to_plot, prop, 'prob_fixation', density_threshold=50, with_violin=True, color_dict=category_color_dict, highlight_categories=animal_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hybrid_density(df_to_plot, 'degree_std', 'mean_steps', density_threshold=50, with_violin=False, color_dict=category_color_dict)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hexbin(df_to_plot['degree_std'], df_to_plot['mean_steps'], gridsize=20, cmap='YlOrRd', mincnt=1)\n",
    "plt.xlabel('degree_std')\n",
    "plt.ylabel('mean_steps')\n",
    "plt.colorbar(label='count')\n",
    "plt.title('Hexbin plot: degree_std vs mean_steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Let's compare the extreme graphs to the LR and XGBOOST models predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Constants & Paths ---\n",
    "ML_MODELS_DIR = Path('/home/labs/pilpel/matanyaw/moran-process/simulation_data/batch_large_test_30_02/ml_models')\n",
    "\n",
    "TIME_LR_MODEL = \"LR Fixation Time\"\n",
    "TIME_XGBOOST_MODEL = \"XGBOOST Fixation Time\"\n",
    "PROB_LR_MODEL = \"LR Fixation Probability\"\n",
    "PROB_XGBOOST_MODEL = \"XGBOOST Fixation Probability\"\n",
    "\n",
    "# --- 1. Load Models & Features ---\n",
    "models = {\n",
    "    TIME_LR_MODEL: joblib.load(ML_MODELS_DIR / 'mean_steps_linear_regression_pipeline.joblib'),\n",
    "    TIME_XGBOOST_MODEL: joblib.load(ML_MODELS_DIR / 'mean_steps_xgboost_model.joblib'),\n",
    "    PROB_LR_MODEL: joblib.load(ML_MODELS_DIR / 'prob_fixation_linear_regression_pipeline.joblib'),\n",
    "    PROB_XGBOOST_MODEL: joblib.load(ML_MODELS_DIR / 'prob_fixation_xgboost_model.joblib'),\n",
    "}\n",
    "\n",
    "# All models use the exact same features, so we only need to extract the list once\n",
    "expected_features = list(models[TIME_LR_MODEL].feature_names_in_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Load Graphs & Calculate Properties ---\n",
    "all_graphs = joblib.load(BATCH_DIR / \"tmp\" / \"graph_zoo.joblib\")\n",
    "\n",
    "LR_graphs = [g for g in all_graphs if \"LR\" in g.category]\n",
    "XGBOOST_graphs = [g for g in all_graphs if \"XGBOOST\" in g.category]\n",
    "\n",
    "print(f\"Total graphs loaded: {len(all_graphs)}\")\n",
    "\n",
    "# Assuming calculate_graph_properties() returns a dict that includes 'wl_hash'\n",
    "all_graphs_props = [g.calculate_graph_properties() for g in all_graphs]\n",
    "\n",
    "# LR_props_df = pd.DataFrame([g.calculate_graph_properties() for g in LR_graphs])\n",
    "# XGBOOST_props_df = pd.DataFrame([g.calculate_graph_properties() for g in XGBOOST_graphs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Extract True Targets ---\n",
    "# Pull only the columns we need to match against from your analysis table\n",
    "targets_df = analysis_df[['wl_hash', 'mean_steps', 'prob_fixation', 'category']]\n",
    "\n",
    "# --- 4. Merge Features and Targets Safely ---\n",
    "# This locks the computed features to the true targets based exclusively on wl_hash\n",
    "all_graphs_merged = pd.DataFrame(all_graphs_props).merge(targets_df, on='wl_hash', how='inner')\n",
    "\n",
    "# LR_merged = LR_props_df.merge(targets_df, on='wl_hash', how='inner')\n",
    "# XGBOOST_merged = XGBOOST_props_df.merge(targets_df, on='wl_hash', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 5. Clean Missing Values (Required for LR) ---\n",
    "# We must drop rows where ANY required feature or target is NaN\n",
    "cols_to_check = expected_features + ['mean_steps', 'prob_fixation']\n",
    "\n",
    "all_graphs_clean = all_graphs_merged.dropna(subset=cols_to_check)\n",
    "# LR_clean = LR_merged.dropna(subset=cols_to_check)\n",
    "# XGBOOST_clean = XGBOOST_merged.dropna(subset=cols_to_check)\n",
    "\n",
    "print(f\"Total graphs ready for prediction: {len(all_graphs_clean)} (Dropped {len(all_graphs_merged) - len(all_graphs_clean)} NaNs)\")\n",
    "# print(f\"LR graphs ready for prediction: {len(LR_clean)} (Dropped {len(LR_merged) - len(LR_clean)} NaNs)\")\n",
    "# print(f\"XGBOOST graphs ready for prediction: {len(XGBOOST_clean)} (Dropped {len(XGBOOST_merged) - len(XGBOOST_clean)} NaNs)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Split into X (Features) and y (True Targets) ---\n",
    "X = all_graphs_clean[expected_features]\n",
    "# LR_X = LR_clean[expected_features]\n",
    "\n",
    "y_time_true = all_graphs_clean['mean_steps']\n",
    "y_prob_true = all_graphs_clean['prob_fixation']\n",
    "# LR_y_time_true = LR_clean['mean_steps']\n",
    "# LR_y_prob_true = LR_clean['prob_fixation']\n",
    "\n",
    "\n",
    "# XGBOOST_X = XGBOOST_clean[expected_features]\n",
    "# XGBOOST_y_time_true = XGBOOST_clean['mean_steps']\n",
    "# XGBOOST_y_prob_true = XGBOOST_clean['prob_fixation']\n",
    "\n",
    "# --- 7. Predict ---\n",
    "\n",
    "LR_pred_time = models[TIME_LR_MODEL].predict(X)\n",
    "LR_pred_prob = models[PROB_LR_MODEL].predict(X)\n",
    "\n",
    "XGBOOST_pred_time = models[TIME_XGBOOST_MODEL].predict(X)\n",
    "XGBOOST_pred_prob = models[PROB_XGBOOST_MODEL].predict(X)\n",
    "# LR_pred_time = models[TIME_LR_MODEL].predict(LR_X)\n",
    "# LR_pred_prob = models[PROB_LR_MODEL].predict(LR_X)\n",
    "\n",
    "# XGBOOST_pred_time = models[TIME_XGBOOST_MODEL].predict(XGBOOST_X)\n",
    "# XGBOOST_pred_prob = models[PROB_XGBOOST_MODEL].predict(XGBOOST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "\n",
    "# --- 8. Prepare Data for Plotting ---\n",
    "# Bind the predictions back into the clean dataframe\n",
    "plot_df = all_graphs_clean.copy()\n",
    "plot_df['LR_pred_time'] = LR_pred_time\n",
    "plot_df['XGBOOST_pred_time'] = XGBOOST_pred_time\n",
    "plot_df['LR_pred_prob'] = LR_pred_prob\n",
    "plot_df['XGBOOST_pred_prob'] = XGBOOST_pred_prob\n",
    "\n",
    "# --- BULLETPROOF CATEGORY FIX ---\n",
    "# Map the category directly from the loaded Python objects using the hash\n",
    "# This bypasses any category_x / category_y weirdness from the merge!\n",
    "hash_to_cat = {g.wl_hash: g.category for g in all_graphs}\n",
    "plot_df['category'] = plot_df['wl_hash'].map(hash_to_cat)\n",
    "# --- 9. Plot Setup ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "fig.suptitle('Model Performance: Predicted vs. True Values', fontsize=20, y=0.97)\n",
    "\n",
    "def plot_pred_vs_true(ax, df, true_col, pred_col, title, color_dict):\n",
    "    # Scatter plot\n",
    "    sns.scatterplot(\n",
    "        data=df, \n",
    "        x=true_col, \n",
    "        y=pred_col, \n",
    "        hue='category', \n",
    "        palette=color_dict,\n",
    "        alpha=0.7, \n",
    "        edgecolor='w',\n",
    "        s=60,\n",
    "        ax=ax,\n",
    "        legend=False  # <-- NEW: Turn off local subplot legends!\n",
    "    )\n",
    "    \n",
    "    # Calculate R-squared\n",
    "    r2 = r2_score(df[true_col], df[pred_col])\n",
    "    \n",
    "    # Plot the ideal y=x line (diagonal)\n",
    "    min_val = min(df[true_col].min(), df[pred_col].min())\n",
    "    max_val = max(df[true_col].max(), df[pred_col].max())\n",
    "    buffer = (max_val - min_val) * 0.05\n",
    "    line_limits = [min_val - buffer, max_val + buffer]\n",
    "    \n",
    "    ax.plot(line_limits, line_limits, color='black', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(f'True {true_col.replace(\"_\", \" \").title()}', fontsize=12)\n",
    "    ax.set_ylabel(f'Predicted {true_col.replace(\"_\", \" \").title()}', fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add R2 text box\n",
    "    ax.text(\n",
    "        0.05, 0.95, \n",
    "        f'$R^2$ = {r2:.3f}', \n",
    "        transform=ax.transAxes, \n",
    "        fontsize=12, \n",
    "        verticalalignment='top', \n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor='gray')\n",
    "    )\n",
    "\n",
    "# --- 10. Generate Subplots ---\n",
    "plot_pred_vs_true(axes[0, 0], plot_df, 'mean_steps', 'LR_pred_time', 'Linear Regression: Fixation Time', category_color_dict)\n",
    "plot_pred_vs_true(axes[0, 1], plot_df, 'mean_steps', 'XGBOOST_pred_time', 'XGBoost: Fixation Time', category_color_dict)\n",
    "plot_pred_vs_true(axes[1, 0], plot_df, 'prob_fixation', 'LR_pred_prob', 'Linear Regression: Fixation Probability', category_color_dict)\n",
    "plot_pred_vs_true(axes[1, 1], plot_df, 'prob_fixation', 'XGBOOST_pred_prob', 'XGBoost: Fixation Probability', category_color_dict)\n",
    "\n",
    "# --- 11. Create a Single Master Legend ---\n",
    "# Get the unique categories that actually appear in this specific dataset\n",
    "present_categories = sorted(plot_df['category'].dropna().unique())\n",
    "\n",
    "# Manually create the visual markers for the legend based on your color dict\n",
    "legend_handles = [\n",
    "    mlines.Line2D([], [], marker='o', color='w', markerfacecolor=category_color_dict[cat], \n",
    "                  markersize=10, label=cat) \n",
    "    for cat in present_categories if cat in category_color_dict\n",
    "]\n",
    "\n",
    "# Append the dashed line so the viewer knows what the diagonal line means\n",
    "legend_handles.append(mlines.Line2D([], [], color='black', linestyle='--', alpha=0.6, label='Ideal (y=x)'))\n",
    "\n",
    "# Attach the legend to the FIGURE (not the axes), anchored outside on the right\n",
    "fig.legend(handles=legend_handles, title='Graph Category', loc='center left', bbox_to_anchor=(0.84, 0.5), fontsize=12, title_fontsize=14)\n",
    "\n",
    "# --- 12. Adjust Layout Margins ---\n",
    "# Force the plots to stop at 82% of the figure width, leaving a massive empty space on the right for the legend\n",
    "plt.subplots_adjust(left=0.06, right=0.82, top=0.92, bottom=0.08, wspace=0.15, hspace=0.25)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Moran Process",
   "language": "python",
   "name": "moran-process"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
